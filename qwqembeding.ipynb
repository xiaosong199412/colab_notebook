{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN1JMNjdysqqsP0pt8CLbo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaosong199412/colab_notebook/blob/main/qwqembeding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelscope transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK9e2vz0tawn",
        "outputId": "b18925fb-3512-4625-f535-abf3d4cf72dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modelscope\n",
            "  Downloading modelscope-1.27.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.11/dist-packages (from modelscope) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope) (75.2.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from modelscope) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->modelscope) (2025.4.26)\n",
            "Downloading modelscope-1.27.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: modelscope\n",
            "Successfully installed modelscope-1.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBZGJlletGXg"
      },
      "outputs": [],
      "source": [
        "# Requires transformers>=4.51.0\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "def last_token_pool(last_hidden_states: Tensor,\n",
        "                 attention_mask: Tensor) -> Tensor:\n",
        "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
        "    if left_padding:\n",
        "        return last_hidden_states[:, -1]\n",
        "    else:\n",
        "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
        "        batch_size = last_hidden_states.shape[0]\n",
        "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
        "\n",
        "\n",
        "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
        "    return f'Instruct: {task_description}\\nQuery:{query}'\n",
        "\n",
        "# Each query must come with a one-sentence instruction that describes the task\n",
        "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
        "\n",
        "queries = [\n",
        "    get_detailed_instruct(task, 'What is the capital of China?'),\n",
        "    get_detailed_instruct(task, 'Explain gravity')\n",
        "]\n",
        "# No need to add instruction for retrieval documents\n",
        "documents = [\n",
        "    \"The capital of China is Beijing.\",\n",
        "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
        "]\n",
        "input_texts = queries + documents\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-8B', padding_side='left')\n",
        "model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B')\n",
        "\n",
        "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
        "# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n",
        "\n",
        "max_length = 8192\n",
        "\n",
        "# Tokenize the input texts\n",
        "batch_dict = tokenizer(\n",
        "    input_texts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "batch_dict.to(model.device)\n",
        "outputs = model(**batch_dict)\n",
        "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
        "\n",
        "# normalize embeddings\n",
        "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "scores = (embeddings[:2] @ embeddings[2:].T)\n",
        "print(scores.tolist())\n",
        "# [[0.7493016123771667, 0.0750647559762001], [0.08795969933271408, 0.6318399906158447]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK4ZmTWNweQV",
        "outputId": "33d4ee3f-10f0-4008-fbc0-1f4f4511bcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0266,  0.0471, -0.0159,  ..., -0.0113,  0.0042,  0.0148],\n",
              "        [ 0.0332, -0.0068, -0.0064,  ..., -0.0058, -0.0098,  0.0075],\n",
              "        [-0.0139,  0.0523, -0.0032,  ..., -0.0044,  0.0094,  0.0113],\n",
              "        [ 0.0390,  0.0194, -0.0112,  ..., -0.0096,  0.0064, -0.0007]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUuxmGAnyH9k",
        "outputId": "7e5f029a-b4dd-4a24-e7ec-778b903638b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_dict['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im0uc9qZww2W",
        "outputId": "0a8adc76-233e-46dd-f626-51a03912e0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "MCBBNpJOxD9T",
        "outputId": "4e42795a-690f-4b5e-a814-7c1ccf94c8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:What is the capital of China?',\n",
              " 'Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:Explain gravity',\n",
              " 'The capital of China is Beijing.',\n",
              " 'Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}